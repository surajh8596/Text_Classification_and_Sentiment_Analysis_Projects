{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5e8cc0",
   "metadata": {
    "id": "ff5e8cc0"
   },
   "source": [
    "<h1 style=\"background:lightblue; color:blue; line-height:3; text-align:center; font-family:Arial Black\">* Procedure in NLP *</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c67e26",
   "metadata": {
    "id": "93c67e26"
   },
   "source": [
    "<h3 style=\"color:gray; line-height:1.2;\">1. Import the General libraries, NLP module like NLTK and SPACY.<br>\n",
    "                        2. Load the dataset.<br>\n",
    "                        3. Text Preprocessing:<br>\n",
    "                        &ensp;&ensp;&ensp; i.   Removing html tags <br>\n",
    "                        &ensp;&ensp;&ensp; ii.  Removing Punctuations <br>\n",
    "                        &ensp;&ensp;&ensp; iii. Performing stemming <br>\n",
    "                        &ensp;&ensp;&ensp; iv.  Removing Stop words <br>\n",
    "                        &ensp;&ensp;&ensp; v.   Expanding contractions. <br>\n",
    "                        4. Apply Tokenization. <br>\n",
    "                        5. Apply Stemming. <br>\n",
    "                        6. Apply POS Tagging. <br>\n",
    "                        7. Apply Lemmatization. <br>\n",
    "                        8. Apply label encoding. <br>\n",
    "                        9. Feature Extraction. <br>\n",
    "                        10 Text to Numerical vector conversion:<br>\n",
    "                        &ensp;&ensp;&ensp; i.   Apply BOW(Count-Vectorizer). <br>\n",
    "                        &ensp;&ensp;&ensp; ii.  Apply TFIDF vectorizer. <br>\n",
    "                        &ensp;&ensp;&ensp; iii. Apply Word2Vector vectorizer. <br>\n",
    "                        &ensp;&ensp;&ensp; iv.  Apply Glove. <br>\n",
    "                        11. Data preprocessing. <br>\n",
    "                        12. Model Building. <br></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf963dc",
   "metadata": {
    "id": "fdf963dc"
   },
   "source": [
    "<h1 style=\"background:lightblue; color:blue; line-height:3; text-align:center; font-family:Arial Black\">* Terms Used in NLP *</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e57d05",
   "metadata": {
    "id": "e2e57d05"
   },
   "source": [
    "<h3><div style=\" line-height:1.2\">\n",
    "    <b style=\"color:orange;\">Document &emsp;&emsp;:&ensp; </b><b style=\"color:gray\">Each row in dataset is called Document.</b><br>\n",
    "    <b style=\"color:orange;\">Corpus &nbsp;&emsp;&emsp;&emsp;:&ensp; </b><b style=\"color:gray\">Collection of Documents(all rows) is called Corpus.</b><br>\n",
    "    <b style=\"color:orange;\">Vocabulary &ensp;&emsp;:&ensp; </b><b style=\"color:gray\">Unique Words in Corpus</b><br>\n",
    "    <b style=\"color:orange;\">Segmentation &nbsp;:&ensp; </b><b style=\"color:gray\">Breaking multiple sentences into single individual sentence is called Segmentation.</b><br>\n",
    "    <b style=\"color:orange;\">Tokenization &nbsp;&ensp;:&ensp; </b><b style=\"color:gray\">Process of breaking sentence into Words is called Tokenization and the words are called Tokens.</b><br> \n",
    "    <b style=\"color:orange;\">StopWords &ensp;&emsp;:&ensp; </b><b style=\"color:gray\">Common words used in any language are called Stop-Words</b><br>\n",
    "    <b style=\"color:orange;\">Stemming &emsp;&emsp;:&ensp; </b><b style=\"color:gray\">Process of removing or replacing suffixes of word to get the root or base word is called <br> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Stemming. But sometimes meaning of word will lost.</b><br>\n",
    "    <b style=\"color:orange;\">Lemmatization :&ensp; </b><b style=\"color:gray\">Process of removing or replacing suffixes of word to get the root or base word is called <br> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Lemmatization. Here words have dictionary meaning.</b><br>\n",
    "    <b style=\"color:orange;\">NER Tagging &ensp;&nbsp;:&ensp; </b><b style=\"color:gray\">Process of Adding Tags to each word like \"Person, Place, Currency\" etc. is called NER Tagging.</b><br>\n",
    "     <b style=\"color:orange;\">POS Tagging &ensp;&nbsp;:&ensp; </b><b style=\"color:gray\">Process of Adding Part of Speech Tags to each word is called POS Tagging..</b><br>\n",
    "    <b style=\"color:orange;\">Chunking &emsp;&nbsp;&emsp;:&ensp; </b><b style=\"color:gray\">Process of Conversion of sentence to a flat tree is called Chunking.</b><br>\n",
    "</div></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc83bec",
   "metadata": {
    "id": "9cc83bec"
   },
   "source": [
    "<h1 style=\"background:lightblue; color:blue; line-height:3; text-align:center; font-family:Arial Black\">* Text Pre-Processing Steps *</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c87b7",
   "metadata": {
    "id": "878c87b7"
   },
   "source": [
    "<h3><p style=\"color:gray; line-height:1.1\">Text preprocessing is a crucial step in NLP. Cleaning our text data in order to convert it into a presentable form that is analyzable and predictable for our task is known as text preprocessing.<br>\n",
    "Many steps can be taken in text preprocessing, few steps are,<br>\n",
    "<b style=\"color:orange\">A. Basic Techniques:</b><br>\n",
    "    &ensp;&ensp;1. Lowering Case<br>\n",
    "    &ensp;&ensp;2. Remove Punctuations<br>\n",
    "    &ensp;&ensp;3. Removal of special characters and Numbers<br>\n",
    "    &ensp;&ensp;4. Removal of HTML tags<br>\n",
    "    &ensp;&ensp;5. Removal of URL's<br>\n",
    "    &ensp;&ensp;6. Removal of Extra Spaces<br>\n",
    "    &ensp;&ensp;7. Expanding Contraction<br>\n",
    "    &ensp;&ensp;8. Text Correction<br>\n",
    "<b style=\"color:orange\">B. Advanced Techniques:</b><br>\n",
    "    &ensp;&ensp;1. Apply Tokenization<br>\n",
    "    &ensp;&ensp;2. Stop Word Removal<br>\n",
    "    &ensp;&ensp;3. Apply Stemming<br>\n",
    "    &ensp;&ensp;4. Apply Lemmatization<br>\n",
    "<b style=\"color:orange\">C. More Advanced Techniques:</b><br>\n",
    "    &ensp;&ensp;1. POS(Part Of Speech) Tagging<br>\n",
    "    &ensp;&ensp;2. NER(Name Entity Recognation)<br></p></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee72559",
   "metadata": {
    "id": "0ee72559"
   },
   "source": [
    "<h2 style=\"background:pink; color:blue; line-height:1.7; font-family:Arial Black; text-align:left\">A. Basic Techniques</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506c4c1",
   "metadata": {
    "id": "d506c4c1"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">1. Lowering Case</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be337ed1",
   "metadata": {
    "id": "be337ed1"
   },
   "source": [
    "<h3><p style=\"color:gray; line-height:1.1\"><b style=\"color:black\"> Lowering Case of text is essential step in text preprocessing due to following reasons:</b><br>\n",
    "    &emsp;&emsp; 1. The same words, one in upper case and other in lower case are considered as different words while creating\n",
    "                    BOW, hence lowering add the same value for both the words.<br>\n",
    "    &emsp;&emsp; 2. In TF-IDF CountVectorization techniques the frequency of words is considered with irrespective of the case.<br>\n",
    "    &emsp;&emsp; 3. Lowering decreasing the size of the vocabulary and hence reduce the dimensionality.</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3f823",
   "metadata": {
    "id": "27c3f823",
    "outputId": "c9e886c9-2dc6-41c1-8e64-8a90241e089d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: What is the STEP by step guide to invest In share market in india?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Lowered Sentence: what is the step by step guide to invest in share market in india?\n"
     ]
    }
   ],
   "source": [
    "sentence=\"What is the STEP by step guide to invest In share market in india?\"\n",
    "sentence_lower=str(sentence).lower()\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Lowered Sentence:\", sentence_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052b48e",
   "metadata": {
    "id": "1052b48e"
   },
   "source": [
    "**<code style=\"color:green;\">In the `Original Sentence` we have two `Step` with different cases and same meaning in sentence, after coverting everything to lower both words look similar and we reduced the dimensionality.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2420d7",
   "metadata": {
    "id": "3a2420d7"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">2. Removing Punctuations</h3>\n",
    "<h3><p style=\"color:orange; line-height:1.1\">To remove Punctuations we are going to use python \"String\" library.</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e546b",
   "metadata": {
    "id": "026e546b",
    "outputId": "7e6ea62a-dfd7-4534-b3f8-7f65b2b95ace"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582b504",
   "metadata": {
    "id": "6582b504"
   },
   "source": [
    "**<code style=\"color:green\">Above are the Punctuations in any language</code>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9f8c2",
   "metadata": {
    "id": "92e9f8c2",
    "outputId": "5eb3e1ec-4718-40cd-dccf-7665a1053761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Hello Everyone, this is team Data Dynamos ! We are got an project of Quora Question SImilirity ^ . We are actually happy !! Because we wanted this project * *\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence without Punctuations: Hello Everyone, this is team Data Dynamos We are got an project of Quora Question SImilirity We are actually happy !! Because we wanted this project\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Hello Everyone, this is team Data Dynamos ! We are got an project of Quora Question SImilirity ^ . We are actually happy !! Because we wanted this project * *\"\n",
    "without_punc=[word for word in sentence.split(\" \") if word not in list(punc)]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence without Punctuations:\", \" \".join(without_punc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a86756",
   "metadata": {
    "id": "c8a86756"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">3. Removing Special Characters and Numbers</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace12224",
   "metadata": {
    "id": "ace12224"
   },
   "source": [
    "<h3><p style=\"color:gray; line-height:1.1\">Special Characters and numbers like \"!,@,#,%,^,&,$,+,*, 1 to 9\" have no meaning in the sentence and they do not contribute to any sentence classification. And there is one senario when these special characters attached to any word will considered as different word which is already present in the sentence. eg. \"Shocked\" and \"Shocked!\" considered as different words but we know they have same meaning. Hence its better to remove any special characters there for dimensionality is also reduces.<br>\n",
    "<b style=\"color:orange\">We are going to use python \"re package\" to remove special characters and numbers.</b></p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34015f1",
   "metadata": {
    "id": "f34015f1",
    "outputId": "921c0556-c2a1-4e63-a27e-08cdc39d6154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Find the remainder when  math          math  is divided by       \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence=\"Find the remainder when [math]23^{24}[/math] is divided by 24,23?\"\n",
    "sentence_clean=re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", sentence_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f168f0f6",
   "metadata": {
    "id": "f168f0f6"
   },
   "source": [
    "**<code style=\"color:green\">In `Original Sentence` \"{},[],/,?,^\" are the special characters, `Clean Sentence` contains no special characters and numbers.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d44a7f",
   "metadata": {
    "id": "e1d44a7f"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">4. Removal of HTML Tags</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e157b1",
   "metadata": {
    "id": "64e157b1"
   },
   "source": [
    "<h3><p style=\"color:gray; line-height:1\">When we Scrap data from any website then dataset contains HTML tags. We might face problem if HTML Tags present in our dataset. Hence it prefered to remove these tags.</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854cddc",
   "metadata": {
    "id": "f854cddc",
    "outputId": "965cbde3-25dd-4609-f59f-73ec2bd6ba34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: <h3 style=\"color:red; font-family:Arial Black\">Hello Guys How Are You</h3>\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Hello Guys How Are You\n"
     ]
    }
   ],
   "source": [
    "sentence='''<h3 style=\"color:red; font-family:Arial Black\">Hello Guys How Are You</h3>'''\n",
    "clean_sentence=re.sub(\"<.*?>\", \"\", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5d40a",
   "metadata": {
    "id": "46f5d40a"
   },
   "source": [
    "**<code style=\"color:green;\">The Original Sentence contains HTML tags, after removing these tags using re.sub function of python regex, our Sentence looks human readable.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc8f23",
   "metadata": {
    "id": "02cc8f23"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">5. Removing URL's</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddcdab",
   "metadata": {
    "id": "b8ddcdab"
   },
   "source": [
    "<h3><p style=\"color:gray; line-height:1\">Some times in the Quora question people provide some external links and url's. As we know that the urls are the random combinations of strings which does not cotains any specific meaning. Hence is useful to remove thes urls.</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7f5c0",
   "metadata": {
    "id": "d8a7f5c0",
    "outputId": "afd5f8ad-81c8-474b-b96d-381a314b0cc3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I visited https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Sentiment%20Analysis link and I found very interesting sentiment analysis projects.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: I visited  link and I found very interesting sentiment analysis projects.\n"
     ]
    }
   ],
   "source": [
    "sentence=\"I visited https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Sentiment%20Analysis link and I found very interesting sentiment analysis projects.\"\n",
    "clean_sentence=re.sub(\"(http|https|www)\\S+\", \"\", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e60bd",
   "metadata": {
    "id": "962e60bd"
   },
   "source": [
    "**<code style=\"color:green\">Original sentence conatins an external website link, which cause problem in our analysis. So after removing this link check the clean sentence, with no url.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb545d96",
   "metadata": {
    "id": "fb545d96"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">6. Removing Extra Spaces</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53fc2eb",
   "metadata": {
    "id": "d53fc2eb"
   },
   "source": [
    "<h3><p style=\"color:gray; line-height:1\">There is some senario where users insert extra spaces at the start, at the end or at the anywhere in the sentence. We need to remove all the extra spaces inserted by an user.</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acf6a0",
   "metadata": {
    "id": "92acf6a0",
    "outputId": "a8b6d504-a998-4489-cdee-f8b25a599616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Hi Team    Data Dynamos, How is your     project going on            ?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Hi Team Data Dynamos, How is your project going on ?\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Hi Team    Data Dynamos, How is your     project going on            ?\"\n",
    "clean_sentence=re.sub(\" +\",\" \", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19abd37",
   "metadata": {
    "id": "e19abd37"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">7. Expanding Contraction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd94f4d",
   "metadata": {
    "id": "0fd94f4d"
   },
   "source": [
    "<h3><p style=\"color:gray; line-height:1.2\">Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe. Nowadays, where everything is shifting online, we communicate with others more through text messages or posts on different social media like Facebook, Instagram, Whatsapp, Twitter, LinkedIn, etc. in the form of texts. With so many people to talk, we rely on abbreviations and shortened form of words for texting people.<br><br>\n",
    "<b style=\"color:orange\">We need to exapnd these contractions so that we can easliy apply tokenization and normalization(stemming and lemmatization). Here we are going to use <b style=\"color:green\">contrations</b> python library to exapand the constraction words.</b></p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505037d",
   "metadata": {
    "id": "4505037d"
   },
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f224ea0",
   "metadata": {
    "id": "7f224ea0",
    "outputId": "8a562345-fd70-4426-f62b-897f9843f08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: We've reached final step of our data science internship. We'll meet u in project presentation.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clear Sentence: We have reached final step of our data science internship. We will meet you in project presentation.\n"
     ]
    }
   ],
   "source": [
    "sentence=\"We've reached final step of our data science internship. We'll meet u in project presentation.\"\n",
    "clear_sentence=contractions.fix(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clear Sentence:\", clear_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd019559",
   "metadata": {
    "id": "dd019559"
   },
   "source": [
    "**<code style=\"color:green\">Original Sentence contains contraction words like `\"We've\",\"We'll\",\"u\"`. And the expanded words for these constraction are `\"We have\",\"We will\", \"You\"`.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b4c18",
   "metadata": {
    "id": "4c8b4c18"
   },
   "source": [
    "<p> <h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">8. Text Correction</h3>\n",
    "<h3 style=\"color:orange\">To correct the text we are going to use TextBlob from NLTK</h3></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07535483",
   "metadata": {
    "id": "07535483",
    "outputId": "dfcc1712-1065-4e91-c147-4f772e267813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: We have reachedd final step of our data science Trainig. We'll meet youu in project presentatiom.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Correct Sentence: He have reached final step of our data science Training. He'll meet you in project presentation.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "sentence=\"We have reachedd final step of our data science Trainig. We'll meet youu in project presentatiom.\"\n",
    "textblob=TextBlob(sentence)\n",
    "correct_sentence=textblob.correct()\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Correct Sentence:\", correct_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448cd94",
   "metadata": {
    "id": "9448cd94"
   },
   "source": [
    "<h2 style=\"background:pink; color:blue; line-height:1.7; font-family:Arial Black; text-align:left\">B. Advanced Techniques</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc88b78",
   "metadata": {
    "id": "1fc88b78"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">1. Apply Tokenization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81290a01",
   "metadata": {
    "id": "81290a01"
   },
   "source": [
    "<h3><p style=\"color:gray; line-height:1.1\">Tokenization is a process of breaking down sentence into words. These words are called Tokens. Here, tokens can be either words, characters, or subwords.<br>\n",
    "<b style=\"color:orange\">Tokenization is broadly classified into 3 types:</b><br>\n",
    "    &ensp;&ensp;a. Sentence Tokenization<br>\n",
    "    &ensp;&ensp;b. Word Tokenization<br>\n",
    "    &ensp;&ensp;c. SubWord(n-gram characters) Tokenization<br><br>\n",
    "<b style=\"color:orange\">Here we can use string \"Split\" method for word tokenization only. For Charcter and SubWord Tokenization we need to use \"NLTK\" inbuit funvtion.</b></p></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f28d38",
   "metadata": {
    "id": "48f28d38"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">a. Sentence Tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830380d9",
   "metadata": {
    "id": "830380d9",
    "outputId": "ba6d0728-190a-494f-a676-126f7b9d8967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence Tokens: ['Our Team name is Team Data Dynamos and we have selected Quora question similarity project.', 'We have started working on this project from 13th of May only.', 'Working with team gives little extra space to apply new things.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentence='''Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.'''\n",
    "tokens=sent_tokenize(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed1fec",
   "metadata": {
    "id": "beed1fec"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">b. Word Tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332cf71a",
   "metadata": {
    "id": "332cf71a",
    "outputId": "6b0ae3d5-bfef-4f47-bc0c-23a7426d0565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Word Tokens: ['Our', 'Team', 'name', 'is', 'Team', 'Data', 'Dynamos', 'and', 'we', 'have', 'selected', 'Quora', 'question', 'similarity', 'project.?']\n"
     ]
    }
   ],
   "source": [
    "sentence='''Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?'''\n",
    "tokens=sentence.split(\" \")\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Word Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472485fb",
   "metadata": {
    "id": "472485fb",
    "outputId": "a0236ca1-2dea-4809-9726-77cac32e99ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Word Tokens: ['Our', 'Team', 'name', 'is', 'Team', 'Data', 'Dynamos', 'and', 'we', 'have', 'selected', 'Quora', 'question', 'similarity', 'project', '.', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sentence='''Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?'''\n",
    "tokens=word_tokenize(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Word Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892c2b6",
   "metadata": {
    "id": "b892c2b6"
   },
   "source": [
    "**<code style=\"color:green\">We can easily see the difference, when we tokenize using string method, it will consider all the special characters & punctuation attached to a word as a part of that word, but when we tokenize using NLTK word_tokenizer it consider those special characters & punctuation as a seperate toke.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca250a",
   "metadata": {
    "id": "98ca250a"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">c. Sub-Word(n-gram character) Tokenization</h3>\n",
    "<h3 style=\"color:gray; line-height:1.1\">N-grams are continuous sequences of words or symbols, or tokens in a document. In technical terms, they can be defined as the neighboring sequences of items in a document.<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae208ccd",
   "metadata": {
    "id": "ae208ccd"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eec001",
   "metadata": {
    "id": "26eec001",
    "outputId": "b0c25e5c-627a-4ab6-8ef3-f26c0779e021",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "N-gram Tokens: [('Our', 'Team', 'name'), ('Team', 'name', 'is'), ('name', 'is', 'Team'), ('is', 'Team', 'Data'), ('Team', 'Data', 'Dynamos'), ('Data', 'Dynamos', 'and'), ('Dynamos', 'and', 'we'), ('and', 'we', 'have'), ('we', 'have', 'selected'), ('have', 'selected', 'Quora'), ('selected', 'Quora', 'question'), ('Quora', 'question', 'similarity'), ('question', 'similarity', 'project.'), ('similarity', 'project.', 'We'), ('project.', 'We', 'have'), ('We', 'have', 'started'), ('have', 'started', 'working'), ('started', 'working', 'on'), ('working', 'on', 'this'), ('on', 'this', 'project'), ('this', 'project', 'from'), ('project', 'from', '13th'), ('from', '13th', 'of'), ('13th', 'of', 'May'), ('of', 'May', 'only.'), ('May', 'only.', 'Working'), ('only.', 'Working', 'with'), ('Working', 'with', 'team'), ('with', 'team', 'gives'), ('team', 'gives', 'little'), ('gives', 'little', 'extra'), ('little', 'extra', 'space'), ('extra', 'space', 'to'), ('space', 'to', 'apply'), ('to', 'apply', 'new'), ('apply', 'new', 'things.')]\n"
     ]
    }
   ],
   "source": [
    "sentence='''Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.'''\n",
    "n_gram_tokens=list(ngrams((sentence.split(\" \")), n=3))\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"N-gram Tokens:\", n_gram_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1471d29",
   "metadata": {
    "id": "b1471d29"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">2. Remove Stop Words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc249474",
   "metadata": {
    "id": "dc249474",
    "outputId": "3a149c5a-dd01-4dba-df1e-a31279175f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Stop Words in English= 179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en=stopwords.words(\"english\")\n",
    "print(\"Total Stop Words in English=\", len(stopwords_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8541c9c",
   "metadata": {
    "id": "b8541c9c"
   },
   "source": [
    "**<code style=\"color:green\">English language contains 179 Stop WOrds.</code>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c76a0",
   "metadata": {
    "id": "458c76a0",
    "outputId": "cedb9e25-7c24-4761-ac13-efd1fd745a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with StopWOrds: Our Team name is Team Data Dynamos and we have selected Quora question similarity project\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence without StopWOrds: Our Team name Team Data Dynamos selected Quora question similarity project\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Our Team name is Team Data Dynamos and we have selected Quora question similarity project\"\n",
    "sentence_non_stopword=[word for word in sentence.split(\" \") if not word in stopwords_en]\n",
    "print(\"Sentence with StopWOrds:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence without StopWOrds:\", \" \".join(sentence_non_stopword))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700af43",
   "metadata": {
    "id": "a700af43"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">3. Apply Stemming</h3>\n",
    "<h3><p style=\"color:orange;\"><b>Types of Stemmer in NLP:</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">a. Porter Stemmer</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">b. SnowBall Stemmer</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">c. Lancaster Stemmer</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">d. Regexp Stemmer</b><br>\n",
    "</p></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a9ccb",
   "metadata": {
    "id": "a22a9ccb"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">a. Porter Stemmer</h3>\n",
    "<h3 style=\"color:gray\">Porter Stemmer is the original stemmer but the stem sometimes illogical or non-dictionary word.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5ee45",
   "metadata": {
    "id": "90e5ee45",
    "outputId": "8a10b6bb-81d6-4821-c843-4ac594cc68d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: connect connect connect connect connect connect connect drive driven drive abl abl enabl enabl enabl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\"\n",
    "porter_stem=[porter.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(porter_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d703d0c",
   "metadata": {
    "id": "5d703d0c"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">b. Snowball Stemmer</h3>\n",
    "<h3 style=\"color:gray\">Snowball stemmer is faster and more logical than the Porter Stemmer.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3243fba2",
   "metadata": {
    "id": "3243fba2",
    "outputId": "57225fe7-83ea-49c1-a8b9-c4e8a70a25d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: connect connect connect connect connect connect connect drive driven drive abl abl enabl enabl enabl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball=SnowballStemmer(language=\"english\")\n",
    "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\"\n",
    "snowball_stem=[snowball.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(snowball_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cc40e",
   "metadata": {
    "id": "7a0cc40e"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">c. Lancaster Stemmer</h3>\n",
    "<h3 style=\"color:gray\">The Lancaster stemmers are more aggressive and dynamic. The stemmer is really faster, but the algorithm is really confusing when dealing with small words. Lancaster Stemmer produces results with excessive stemming.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ba3d8",
   "metadata": {
    "id": "de4ba3d8",
    "outputId": "beeaaaba-243d-4491-beb2-34d4200aeec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: connect connect connect connect connect connect connect driv driv driv abl abl en en en\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\"\n",
    "lancaster_stem=[lancaster.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(lancaster_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873ad43",
   "metadata": {
    "id": "3873ad43"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">d. Regexp Stemmer</h3>\n",
    "<h3 style=\"color:gray\">Regexp stemmer identifies morphological affixes using regular expressions. Substrings matching the regular expressions will be discarded.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b72bf",
   "metadata": {
    "id": "ae7b72bf",
    "outputId": "1e624edd-cf45-44fa-c9c9-e31a3aa7d827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: Connect Connection Connection Connect Connected Connect Connecting Driv Driven Drive Abl Able Enabl Enable Enabl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regex=RegexpStemmer(regexp=\"ing$|s$|e$\", min=0)\n",
    "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\"\n",
    "regex_stem=[regex.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(regex_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cf5c6",
   "metadata": {
    "id": "f98cf5c6"
   },
   "source": [
    "**<code style=\"color:green\">All Stemmers are Different from each other. Ther is one common thing between all stemmers, sometimes they did not return the stem with logical or dictionary meaning.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf45fb",
   "metadata": {
    "id": "dabf45fb"
   },
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">4. Apply Lemmatization</h3>\n",
    "<h3><p style=\"color:orange;\"><b>Types of Lemmatization in NLP:</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">a. Wordnet Lemmatizer</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">b. TextBlob Lemmatizer</b><br>\n",
    "</p></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c25f7",
   "metadata": {
    "id": "c10c25f7"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">a. Wordnet Lemmatizer</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4886625",
   "metadata": {
    "id": "c4886625",
    "outputId": "5fc3056f-cbb1-4d83-ced6-df08b7a2b614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The bats are hanging on their feet in upright positions\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Lemmatization: The bat be hang on their feet in upright position\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "sentence=\"The bats are hanging on their feet in upright positions\"\n",
    "sentence_lemma=[lemma.lemmatize(word, 'v') for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Lemmatization:\", \" \".join(sentence_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94858bc8",
   "metadata": {
    "id": "94858bc8"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">b. TextBlob Lemmatizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8725ecc",
   "metadata": {
    "id": "c8725ecc",
    "outputId": "8c0d7d62-e767-4354-8a78-034f4b620d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The bats are hanging on their feet in upright positions\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Lemmatization: The bat are hanging on their foot in upright position\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "sentence=\"The bats are hanging on their feet in upright positions\"\n",
    "sent=TextBlob(sentence)\n",
    "texblob_lemma=[w.lemmatize() for w in sent.words]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Lemmatization:\", \" \".join(texblob_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa87baf",
   "metadata": {
    "id": "cfa87baf"
   },
   "source": [
    "<h2 style=\"background:pink; color:blue; line-height:1.7; font-family:Arial Black; text-align:left\">C. More Advanced Techniques</h2>\n",
    "<p><h3 style=\"color:gray;\">These Techniques are not used in all the tasks, these are problem specific. These techniques are mainly used in QA System(Question Answer), Word Sense Disambiguiation etc.</h3>\n",
    "<h3 style=\"color:orange;\">More Advanced Techniques are:<br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">1. POS(Part of Speech) Tagging</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">2. NER(Name Entity Recognisation) Tagging</b><br>\n",
    "</h3></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fd7fd",
   "metadata": {
    "id": "9b5fd7fd"
   },
   "source": [
    "<p><h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">1. POS Tagging</h3>\n",
    "<h3 style=\"color:gray;\">Adding a Part of Speech tags to every word in the corpus is called POS tagging. If we want to perform POS tagging then no need to remove stopwords. This is one of the essential steps in the text analysis where we know the sentence structure and which word is connected to the other, which word is rooted from which, eventually, to figure out hidden connections between words which can later boost the performance of our Machine Learning Model.<br>\n",
    "    <b style=\"color:orange\">POS Tagging can be performed using two Libraries</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">a. POS Tagging using NLTK</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">b. POS Tagging using Spacy</b><br>\n",
    "</h3></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c7cb2",
   "metadata": {
    "id": "a09c7cb2"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">a. POS Tagging using NLTK</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61bfc5e",
   "metadata": {
    "id": "f61bfc5e",
    "outputId": "b1b8ba09-f206-45e3-9e99-c27d667bee7c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: What || POS Tag: WP\n",
      "Word: is || POS Tag: VBZ\n",
      "Word: the || POS Tag: DT\n",
      "Word: step || POS Tag: NN\n",
      "Word: by || POS Tag: IN\n",
      "Word: step || POS Tag: NN\n",
      "Word: guide || POS Tag: RB\n",
      "Word: to || POS Tag: TO\n",
      "Word: invest || POS Tag: VB\n",
      "Word: in || POS Tag: IN\n",
      "Word: share || POS Tag: NN\n",
      "Word: market || POS Tag: NN\n",
      "Word: in || POS Tag: IN\n",
      "Word: india || POS Tag: NN\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "doc=word_tokenize(\"What is the step by step guide to invest in share market in india\")\n",
    "for i in range(len(doc)):\n",
    "    print(\"Word:\",pos_tag(doc)[i][0], \"||\", \"POS Tag:\", pos_tag(doc)[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb7ccf",
   "metadata": {
    "id": "3ecb7ccf"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">b. POS Tagging using Spacy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c546395",
   "metadata": {
    "id": "3c546395"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af661e4c",
   "metadata": {
    "id": "af661e4c",
    "outputId": "fe20d593-041b-4c66-e946-c5a0e75d37a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: What || POS: PRON || POS Tag: WP || Explanation: wh-pronoun, personal\n",
      "Word: is || POS: AUX || POS Tag: VBZ || Explanation: verb, 3rd person singular present\n",
      "Word: the || POS: DET || POS Tag: DT || Explanation: determiner\n",
      "Word: step || POS: NOUN || POS Tag: NN || Explanation: noun, singular or mass\n",
      "Word: by || POS: ADP || POS Tag: IN || Explanation: conjunction, subordinating or preposition\n",
      "Word: step || POS: NOUN || POS Tag: NN || Explanation: noun, singular or mass\n",
      "Word: guide || POS: NOUN || POS Tag: NN || Explanation: noun, singular or mass\n",
      "Word: to || POS: PART || POS Tag: TO || Explanation: infinitival \"to\"\n",
      "Word: invest || POS: VERB || POS Tag: VB || Explanation: verb, base form\n",
      "Word: in || POS: ADP || POS Tag: IN || Explanation: conjunction, subordinating or preposition\n",
      "Word: share || POS: NOUN || POS Tag: NN || Explanation: noun, singular or mass\n",
      "Word: market || POS: NOUN || POS Tag: NN || Explanation: noun, singular or mass\n",
      "Word: in || POS: ADP || POS Tag: IN || Explanation: conjunction, subordinating or preposition\n",
      "Word: india || POS: PROPN || POS Tag: NNP || Explanation: noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"What is the step by step guide to invest in share market in india\")\n",
    "for word in doc:\n",
    "    print(\"Word:\", word.text,\"||\",\"POS:\", word.pos_, \"||\", \"POS Tag:\", word.tag_, \"||\", \"Explanation:\", spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210abf8",
   "metadata": {
    "id": "4210abf8"
   },
   "source": [
    "**<code style=\"color:green\">Spacy is more powerful than NLTK. Spacy is faster and Grammatically accurate.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce7308",
   "metadata": {
    "id": "6dce7308"
   },
   "source": [
    "<p><h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">2. NER Tagging</h3>\n",
    "<h3 style=\"color:gray;\">Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important information in text known as named entities. Named entities refer to the key subjects of a piece of text, such as names, locations, companies, events and products, as well as themes, topics, times, monetary values and percentages.<br>\n",
    "    <b style=\"color:orange\">NER can be performed using two Libraries</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">a. NER using NLTK</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">b. NER using Spacy</b><br>\n",
    "</h3></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739e13a",
   "metadata": {
    "id": "8739e13a"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">a. NER using NLTK</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7e6de",
   "metadata": {
    "id": "45c7e6de"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords_en=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea46d60",
   "metadata": {
    "id": "4ea46d60",
    "outputId": "1f4774f2-15b5-4772-fd86-b848b00854c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ORGANIZATION TATA/NNP Mahindra/NNP)\n",
      "('top', 'JJ')\n",
      "('companies', 'NNS')\n",
      "('India.', 'NNP')\n",
      "('But', 'CC')\n",
      "(\"'Gautam\", 'NNP')\n",
      "(\"Adani'\", 'NNP')\n",
      "(\"'Mukesh\", 'POS')\n",
      "(\"Ambani'\", 'NNP')\n",
      "('reachest', 'NN')\n",
      "('person.', 'NN')\n"
     ]
    }
   ],
   "source": [
    "sentence=\"TATA and Mahindra are the top companies in India. But the 'Gautam Adani' and 'Mukesh Ambani' are the reachest person.\"\n",
    "words=[word for word in sentence.split(\" \") if word not in stopwords_en]\n",
    "tagged_tokens=nltk.pos_tag(words)\n",
    "entities=nltk.ne_chunk(tagged_tokens)\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48efe7b",
   "metadata": {
    "id": "c48efe7b"
   },
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">b. NER using Spacy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d56c6",
   "metadata": {
    "id": "046d56c6",
    "outputId": "898591a0-2fdd-4c8e-c232-98e354457e8d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TATA ORG\n",
      "Mahindra ORG\n",
      "India GPE\n",
      "Gautam Adani' PERSON\n",
      "Mukesh Ambani' PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence=\"TATA and Mahindra are the top companies in India. But the 'Gautam Adani' and 'Mukesh Ambani' are the reachest person.\"\n",
    "doc = nlp(sentence)\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d96e1",
   "metadata": {
    "id": "893d96e1"
   },
   "source": [
    "**<code style=\"color:green\">Spacy is a faster and more efficient library for NER. It provides a pre-trained NER model that is highly accurate than NLTK and can recognize a wide range of named entities. Additionally, SpaCy has more advanced features such as named entity linking and coreference resolution.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdeda6e",
   "metadata": {
    "id": "bfdeda6e"
   },
   "source": [
    "<h1 style=\"background:lightblue; color:blue; line-height:3; text-align:center; font-family:Arial Black\">* Text to Numerical Vector Conversion Techniques *</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33b275",
   "metadata": {
    "id": "af33b275"
   },
   "source": [
    "<p><h3 style=\"color:gray; line-height:1.2\">Our Machine Learning and Deep Learning models take only numerical data as an input to train the model and do prediction, Hence it is necessary to perform conversion step to make texual data into equivalent numerical representation. There are many text to numerical vector conversion techniques, these techniques are,</h3></p>\n",
    "<h3 style=\"color:gray; line-height:1.2\"><b style=\"color:orange\">1. BOW(Bag Of Word): Count Vectorizer</b><br>\n",
    "<b style=\"color:orange\">2. TF-IDF(Term Frequence-Inverse Document Frequency)</b><br>\n",
    "<b style=\"color:orange\">3. Word2Vec(Word to Vector)</b><br>\n",
    "<b style=\"color:orange\">4. GloVe(Global Vector)</b><br>\n",
    "<b style=\"color:orange\">5. BERT(Bidirectional Encoder Representations from Transformers) </b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456aad74",
   "metadata": {
    "id": "456aad74"
   },
   "source": [
    "<p><h2 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">1. Bag Of Word(Count Vectorizer)</h2>\n",
    "<h3 style=\" color:gray; line-height:1.3\">It is a Collection of words represent a sentence with word count. Steps invloved in this process are Clean Text, Tookenize, Build Vocabulary and Generate Vecors. We can create vocabulory of size 1 to n using uni-ngram, bi-gram, n-gram.<br>\n",
    "<b style=\"color:orange\">Advantages:</b><br>\n",
    "    &emsp;&emsp; a. Simple Procedure and easy to implement.<br>\n",
    "    &emsp;&emsp; b. Easy to Understand<br>\n",
    "<b style=\"color:orange\">Disadvantages:</b><br>\n",
    "    &emsp;&emsp; a. Does not consider the symmentic meaning of the word.<br>\n",
    "    &emsp;&emsp; b. Due to large vector size computational time is high.<br>\n",
    "    &emsp;&emsp; c. Count Vectorizer Generates Spars matrix.<br>\n",
    "    &emsp;&emsp; d. Out of Vocabulary words are not captured.</h3><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55420915",
   "metadata": {
    "id": "55420915"
   },
   "source": [
    "<p><h2 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">2. TF-IDF(Term Frequence-Inverse Document Frequency)</h2>\n",
    "<h3 style=\" color:gray; line-height:1.3\">It is a Statistical method. It measures how important a term or word is within a document or setence relative to a collection of documents or Corpus. Words within a text document are transformed into importance numbers by a text vectorization process.<br>\n",
    "<b style=\"color:orange\">Advantages:</b><br>\n",
    "    &emsp;&emsp; a. Simple Procedure and easy to implement.<br>\n",
    "    &emsp;&emsp; b. Easy to Understand<br>\n",
    "    &emsp;&emsp; c. Here unlike BOW, weightage for those words is given high if that word occuring in that document but occuring less in corpus.<br>\n",
    "<b style=\"color:orange\">Disadvantages:</b><br>\n",
    "    &emsp;&emsp; a. Does not consider the symmentic meaning of the word.<br>\n",
    "    &emsp;&emsp; b. Due to large vector size computational time is high.<br>\n",
    "    &emsp;&emsp; c. Count Vectorizer Generates Spars matrix.<br>\n",
    "    &emsp;&emsp; d. Out of Vocabulary words are not captured.</h3><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a2ea74",
   "metadata": {
    "id": "30a2ea74"
   },
   "source": [
    "<h2 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">3. Word2Vec(Word to Vector)</h2>\n",
    "<h3 style=\" color:gray; line-height:1.3\">It is a pre-trained word embedded model. Word2Vec creates vectors of the words that are distributed numerical representations of word features. These word features represents the context for the each words present in vocabulary. Two different model architectures that can be used by Word2Vec to create the word embeddings are the Continuous Bag of Words (CBOW) model(Used when dataset is small) & the Skip-Gram model(Used when the dataset is large).<br>\n",
    "<b style=\"color:orange\">Advantages:</b><br>\n",
    "    &emsp;&emsp; a. Word embeddings eventually help in establishing the association of a word with another similar meaning word through the created vectors.<br>\n",
    "    &emsp;&emsp; b. Captures symmantic meaning.<br>\n",
    "    &emsp;&emsp; c. Low Dimensional vectors hence the computational time reduces.<br>\n",
    "    &emsp;&emsp; d. Dense vectors.<br>\n",
    "<b style=\"color:orange\">Disadvantages:</b><br>\n",
    "    &emsp;&emsp; a. Contexual meaning only captured within the window size. or in other word it has local context scope.<br>\n",
    "    &emsp;&emsp; b. Not able to generate vectors for unseen words.</h3><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc193e39",
   "metadata": {
    "id": "cc193e39"
   },
   "source": [
    "<h2 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">4. GloVe(Global Vector)</h2>\n",
    "<h3 style=\" color:gray; line-height:1.3\">It is also a Pre-trained word embedding technique used to overcome drawback of Word2Vec. <br>\n",
    "<b style=\"color:orange\">Advantages:</b><br>\n",
    "    &emsp;&emsp; a. Contexual meaning captured for both local and global scope.<br>\n",
    "    &emsp;&emsp; b. It uses co-occurance matrix to tell us how often two words occuring together.<br>\n",
    "    &emsp;&emsp; c. Captures symmantic meaning.<br>\n",
    "    &emsp;&emsp; d. Low Dimensional vectors hence the computational time reduces.<br>\n",
    "    &emsp;&emsp; e. Dense vectors.<br>\n",
    "<b style=\"color:orange\">Disadvantages:</b><br>\n",
    "    &emsp;&emsp; a. Utilizes massive memory and takes time to load.<br></h3><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b80157",
   "metadata": {
    "id": "e5b80157"
   },
   "source": [
    "<h2 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">5. BERT(Bidirectional Encoder Representations from Transformers)</h2>\n",
    "<h3 style=\" color:gray; line-height:1.3\">BERT is the Pre-trained birectional trasformer for Language understanding. It has trained on 2500M Wikipedia words and 800M+ Books words. And BERT used by Google search Engine. BERT uses the encoder part of the Transformer, since its goal is to create a model that performs a number of different NLP tasks.<br>\n",
    "<b style=\"color:orange\">Advantages:</b><br>\n",
    "    &emsp;&emsp; a. Contexual meaning captured for both local and global scope.<br>\n",
    "    &emsp;&emsp; b. Captures symmantic meaning.<br>\n",
    "    &emsp;&emsp; c. Powerful than all previous wod embedding techniques.<br>\n",
    "<b style=\"color:orange\">Disadvantages:</b><br>\n",
    "    &emsp;&emsp; a. Utilizes massive memory and takes time to load and train.<br></h3><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e96269",
   "metadata": {},
   "source": [
    "**There are manay techniques used in NLP, I just listed few basic fundamental steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
